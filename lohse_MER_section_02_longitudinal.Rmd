---
title: 'Section 2: Mixed-Effects Models for Longitudinal Data'
author: "Keith Lohse, PhD, PStat; Mike Strube, PhD; Allan Kozlowksi, PhD, PT"
date: '2022-10-04'
output:
  html_document:
    df_print: paged
  pdf_document: default
---

## Note that this page is in the process of being updated 2022-10-04

Working with a new data set, we now want to switch from modeling time as a factor (i.e., treating Trial 1 as categorically different from Trial 2) to treating time as a continuous variable.  

In our previous data set, we used the hypothetical cross-sectional study of younger and older adults. Both groups (hypothetically) walked in anxiety provoking conditions (let's say we simulated a virtual alligator behind them) that initially led them to walk faster than they normally would. After repeated exposures however (4 trials), both groups started to walk slower. In this experiment, our time variable is the number of trials. Each trial came at approximately the same time for each person, so there is no between-subject variability in the time variable. 


```{r, setting libraries, results="hide", message = FALSE, echo=FALSE, warning=FALSE}
library(tidyverse); library(RCurl); library(ez); library(lme4); library(car); library(lmerTest)

DATA <- read.csv("https://raw.githubusercontent.com/keithlohse/mixed_effects_models/master/data_AGING_example.csv",
                 stringsAsFactors = TRUE)

data_TIME <- DATA %>% group_by(subID, time, age_group, group) %>% # Note we ignore condition to average across it
  summarize(speed = mean(speed, na.rm=TRUE)) %>% # I have included na.rm=TRUE even though there are no missing data
  arrange(age_group, subID, time)
```

``` {r plotting the effects of time, echo=FALSE, fig.align="center", fig.height = 4, fig.width = 7}
ggplot(data_TIME, aes(x = time, y = speed)) +
  geom_line(aes(group=subID), col="black", alpha=0.8)+
  geom_point(aes(fill=subID), pch=21, size=2)+
  facet_wrap(~age_group) +
  scale_x_continuous(name = "Trial") +
  scale_y_continuous(name = "Speed (m/s)") +
  theme(axis.text=element_text(size=16, color="black"), 
        axis.title=element_text(size=16, face="bold"),
        plot.title=element_text(size=16, face="bold", hjust=0.5),
        strip.text = element_text(size=16, face="bold"),
        legend.position = "none")
  
```


For the new example though, we will use some fake data that have a structure more like what we might encounter in a longitudinal study. These fake data follow patients with different types of spinal cord injury (given by "AIS Grade") through 18 months of rehabilitation. The outcome is their Rasch-scaled score on the Functional Independence Measure (FIM). Without getting into the weeds of what that means, the values range from 0 to 100, with 100 indicating complete independence in the activities of daily life. Beyond having a lot more observations per person, this dataset also measures time continuously. That is, rather than trials that were all collected at exactly the same time, these data were all collected on different days for different people. Thus, Month 1 as a time point might be Day 20 for some people, but Day 30 for others. One of the strengths of the mixed-effects model is that we can retain this variability in our $X$ variable, by treating time continuously rather than categorically. 

```{r, reading in second data, results="hide", message = FALSE, echo=FALSE}
DAT2 <- read.csv("https://raw.githubusercontent.com/keithlohse/mixed_effects_models/master/data_SCI_longitudinal_example.csv",
                 stringsAsFactors = TRUE)
```

``` {r plotting rehab data, echo=FALSE, fig.align="center", fig.height = 4, fig.width = 8}
ggplot(DAT2, aes(x = time, y = rasch_FIM)) +
  geom_line(aes(group=subID), col="black", alpha=0.8)+
  geom_point(aes(fill=subID), pch=21, size=2)+
  facet_wrap(~AIS_grade) +
  scale_x_continuous(name = "Time (Months)") +
  scale_y_continuous(name = "Rasch-Scaled FIM") +
  theme(axis.text=element_text(size=16, color="black"), 
        axis.title=element_text(size=16, face="bold"),
        plot.title=element_text(size=16, face="bold", hjust=0.5),
        strip.text = element_text(size=16, face="bold"),
        legend.position = "none")
  
```


We will explore these data in more detail below. One of the key differences between these models and the factorial models we considered before is that now we will need to deal with **model comparisons**. In the factorial designs, we knew which factors we wanted to include in our model and we always tested the fully factorial design (i.e., all main-effects and interactions). In contrast, for these longitudinal models, we will need to compare different ways to represent time (e.g., linear versus curvilinear models). After we compare models to decide on the best to model time, we can start adding in other variables to explain the differences in individual trajectories. 

In order to do this, we will need to use **Full Maximum Likelihood** to estimate the different model parameters rather than **Restricted Maximum Likelihood**. Additionally, we will need to decide on a metric for deciding when one model is statistically better than another model. I will cover these topics briefly below, but I would direct interested readers to Jeff Long's, *Longitudinal Data Analysis for the Behavioral Science using R* for a more detailed discussion.


# 1. Modeling Changes in Functional Independence over Time
Starting with modeling changes in functional independence across 18 months of rehabilitation, we can test a series of **unconditional** random-effects models to decide how we want to model the effect of time. These models are said to be "unconditional" because the effect of time does not depend on any other variables. In this series of models, the goal is to establish the best fitting "shape" of our time variable (e.g., should time be linear or quadratic?) and which random-effects are essential to include in the model. 

In these models, I am first going to convert time from months into years in order to avoid any scaling issues in our models. You could also center time around the average time, which I call *year.c* in the models below ($year_i - \overline{year}$). Centering time around it's mean changes the interpretation of the intercept from being the predicted value when Time is 0, to the predicted value on average over time.
```{r}
# Centering time on the first point and converting to years:
DAT2$year.0 <- (DAT2$time-1)/12
DAT2$year.0_sq<-DAT2$year.0^2
DAT2$year.0_cu<-DAT2$year.0^3

# Centering time on the mean time:
DAT2$year.c <- scale(DAT2$time)
DAT2$year.c_sq<-DAT2$year.c^2
DAT2$year.c_cu<-DAT2$year.c^3
```
Centering time on the first time point and centering time on the mean are equally appropriate choices mathematically, but they do change how we will interpret the intercept, so it is very important to understand how your variables are coded. In general, **treating the first data point as time = 0 makes the intercept more interpretable**. However, **mean-centering (or z-transforming) the time variable can help reduce collinearity in a model, especially when interactions are present**.


## 1.1. Unconditional Models of Time
### 1.1.1. Fixed-Slope Random-Intercepts Model
```{r}
# Random intercepts model ---- 
raneff_00<-lmer(rasch_FIM~
                # Fixed-effects
                1+
                # Random-effects
                (1|subID), data=DAT2, REML=FALSE,
                control=lmerControl(optimizer="bobyqa",
                                    optCtrl=list(maxfun=5e5)))
summary(raneff_00)
```


### 1.1.2. Random-Slopes Random-Intercepts Model
```{r}
# Random slope random intercepts model ---- 
raneff_01<-lmer(rasch_FIM~
                # Fixed-effects
                1+year.0+
                # Random-effects
                (1+year.0|subID), data=DAT2, REML=FALSE,
                control=lmerControl(optimizer="bobyqa",
                                    optCtrl=list(maxfun=5e5)))
summary(raneff_01)
```


### 1.1.3. Quadratic Random-Slopes Random-Intercepts Model
```{r}
# Random quadratic slopes and intercepts model ---- 
raneff_02<-lmer(rasch_FIM~
                # Fixed-effects
                1+year.0+year.0_sq+
                # Random-effects
                (1+year.0+year.0_sq|subID), data=DAT2, REML=FALSE,
                control=lmerControl(optimizer="bobyqa",
                                    optCtrl=list(maxfun=5e5)))
summary(raneff_02)
```

### 1.1.4. Cubic Random-Slopes Random-Intercepts Model
```{r}
# Random cubic slopes and intercepts model ---- 
raneff_03<-lmer(rasch_FIM~
                # Fixed-effects
                1+year.0+year.0_sq+year.0_cu+
                # Random-effects
                (1+year.0+year.0_sq+year.0_cu|subID), data=DAT2, REML=FALSE,
                control=lmerControl(optimizer="bobyqa",
                                    optCtrl=list(maxfun=2e5)))
summary(raneff_03)
```

## 1.2. Comparing between the Different Models
Now that we have created several different models, how do we decide which models are the best? How do we decide which parameters to include and which parameters are "statistically significant"? When it comes to mixed-effect linear models (or other forms of "multi-level" models), we use a similar criterion to traditional regression. That is, we still rely on the general idea that: 
$$ Data_i = Model_i + Error_i $$

... and if the error is reduced by a large enough magnitude, then we will call that effect statistically significant (i.e., the parameter reduced error by an unusually larger amount under the null-hypothesis.). However, there are some key differences between mixed-effect models and traditional Ordinary Least Squares regression that we need to discuss.


For instance, you might have noticed that p-values are conspicuously absent from the LME4 output. The reasons for this are complicated, but it has to do with the fact that these models can have statistical dependencies and unbalanced/missing data that do not make the calculation of traditional p-values tenable. In short, your denominator degrees of freedom can get a little crazy. (You can also read an explanation from Doug Bates, a main author of the lme4 package, here: https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html). However, the key things that you should know are:

1. If you really, really want p-values for individual parameters, you can get them from packages that implement the Welch-Satterthwaite approximation to estimate the appropriate degrees of freedom, like the "lmerTest"" package.

2. We are most interested in comparisons between models and less so the individual parameters within a model. All of our models were fit using Full Maximum Likelihood Estimation and we judge models based on a reduction in ERROR that we call Deviance. Fortunately, there are several quantitative and objective methods for evaluating the change in deviance. We will focus on two of these methods, the Wald Test for the change in Deviance and the Akaike Information Criterion (AIC).

You can read more about Full Maximum Likelihood Estimation from other resources, but conceptually the idea of maximum likelihood estimation is that our computer tests a long series of parameters until it arrives at the specific set of parameters that lead to the smallest amount of error in our data. This is why it is referred to as "maximum likelihood", because we arrive at the set of values (for the given parameters) that are *most likely* to have produced the data we observed. The goodness of fit for these parameter estimates is quantified in  the *Deviance*, which is a transformation of the *Likelihood*.

$$ Deviance = -2*log(Likelihood) $$

Where likelihood is defined by the amount of error ($\epsilon$) left behind by our estimates. Thus, if we delve a little deeper, the deviance is:

$$ Deviance = N*log(2\pi\sigma^2_\epsilon)+(1/\sigma^2_\epsilon)*(\sum(\epsilon^2_i)) $$

This formula is kind of scary, but there are two things you need to notice about it:

1. Looking at the right side of the equation, notice that the deviance is still largely determined by the sum of errors. **Thus, all else being equal, smaller errors are going to lead to a smaller deviance.**

2. Notice that size our sample shows up in two different places (the N at the beginning and the fact that we are summing over N on the right hand side). This means that the deviance is sensitive to the amount of data we are using. **Thus, if we want to compare models based on their deviance, those models need to be based on the same amount of data.**

## 1.3. The Wald Test of the Change in Deviance
Now we are ready to make a comparison between some of our different statistical models by comparing the change in the Deviance. Let's start by comparing the Random Intercepts, Fixed Slopes, and Random Slopes models using the anova() function in R.

```{r}
anova(raneff_00,raneff_01,raneff_02, raneff_03)
```
The anova() function gives a number of valuable pieces of information. First, it explicitly lists the models that are being tested. Below, it gives us the model name and the degrees of freedom for each model. It then gives us the Akaike's Information Criterion (AIC), the related Bayesian Information Criterion (or BIC), the log of the Likelihood (or logLik), and the Deviance.

The Random Intercepts model can act as our "benchmark" against which other models can be compared. For instance, the reduction in deviance from the Random Intercepts (Deviance = 5859.9) to the Random Linear Slopes model (Deviance = 4812.9) is 1041; which is a huge reduction! The change in deviance follows a $\chi^2$ distribution, which allows us to see if model is a statistical signficant improvement beyond the preceeding model.

The based on the Wald Test of the change in deviance, it appears that all of our models are successive improvements, despite the large increases to the degrees of freedom. The linear model is better than the random-intercepts model, $\chi^2(3)=1046.98, p < 0.001$. the quadratic model is better than the linear, $\chi^2(4)=746.46, p < 0.001$, and the cubic model provides an even better fit than the quadratic, $\chi^2(5)=252.32, p < 0.001$. Thus, we would conclude that the cubic model is the "best" unconditional model (assuming $\alpha = 0.05$).


## 1.4. The Akaike Information Criterion (AIC)
The AIC is another method to evaluating model fit that is based on the Deviance. Although we won't get into the details of the math behind it, the importance of the AIC is in the name "Information Criterion". That is, the AIC is all about informativeness which is slightly different from just being the model that produces the smallest deviance. For a model to be informative, the parameters need to generalize to other new datasets, not just provide the best explanation of the current data. Therefore, the AIC introduces a penalty to reduce **over-fitting** of the model:
$$ AIC = Deviance + 2(k) $$

In the formula above, k = number of parameters in the model. Thus, for a parameter to improve the AIC it has to reduce the deviance by >2 times the number of parameters. As mentioned, we won't get into why the magic number of 2(k) seems to work so well, but the key thing to know is that **the AIC imposes a penalty based on the number of parameters and is thus a more conservative test than the Wald Test of the change in Deviance**. We can see this in action by comparing our models in the ANOVA output above. Note that the AIC is reliably larger than the simple deviance by a factor of $2*k$. 

Ultimately, we still arrive at the same conclusion because the cubic model has the lowest AIC. However, it is important that we base this decision on the AIC rather than just the Wald Test of the change in deviance. The reason for this comes back to over-fitting. Note that the cubic model uses 15 degrees of freedom, which is a lot more than the simpler linear model that used only 6. Adding 9 extra parameters is likely to reduce the deviance just by chance, so by introducing a penalty for every additional parameter, we can really make sure that these **additional parameters are pulling their own weight**, so to speak. (For a more detailed explanation of the AIC and it's unique properties for improving generalizability/reducing over-fitting, see Jeff Long's excellent book, *Longitudinal Data Analysis for the Behavioral Sciences using R*). 

As a word of caution, there are no fixed cut-offs for a "statistically significant" change in the AIC although some research has been done exploring how the AIC relates to other measures of effect-size (see Long, 2012). In general, it is a good idea to declare a *minimum change* in the AIC in advance. Keep in mind that *any* reduction in the AIC means that model is explaining more of the deviance than the complexity it has gained in additional parameters. However, we might want to set a minimum difference of 1 whole point or 2 whole points for selecting a "superior" model. For instance, Anderson (2008, chap. 4) describes a $\Delta AIC = 4$ as a "strong" difference in the explanatory power of two models and $\Delta AIC = 8$ as a "very strong" difference. After all, the **exact** AIC would depend on your data conforming to all of your model's assumptions. For that reason, setting a minimum cut-off or 1 or 2 AIC points is useful, because your AIC may not be precise to the level of 0.01 or 0.1. 

For instance, when the sample size is small, there is a good chance that the AIC will not be penalizing additional parameters enough to avoid overfitting. To address this issue, researchers have developed the AIC-corrected, or $AICc$. Assuming that your model is univariate in it's outcome, linear in its parameters, and has an (approximately) normally distributed residuals, then the AICc can be calculated by:
$$ AICc = AIC + (2k^2+2k)/(n-k-1) $$
Where $n$ is the sample size and $k$ is the number of parameters. As such, the AICc is essentially the AIC with an extra penalty on the number of parameters. As the sample size gets larger, the AICc will effectively converge on the AIC. 

These rules-of-thumb should not be treated as hard-cutoffs however, and analysts should treat the AIC continuously. I do think that these guidelines are useful, however, because our models will likely violate our assumptions to some degree. Using integer values of the AIC then, builds in a "buffer" to help us make sure that we are select a model that truly explains more variability than it adds in complexity. 


## 1.5. Creating a Conditional Model of Change over Time
Once we have settled on our unconditional model of change over time, we can start to add factors to our model to try explain individual differences in these trajectories. 

Specifically, we will add the factor of AIS Grade to our model, as well as interactions between AIS Grade and time.  

After creating this conditional model, we can first check to see if it is an improvement above the unconditional models, by comparing their respective AICs using the anova() function.




```{r}
# Effect of AIS Grade on Time
cond_01<-lmer(rasch_FIM~
                  # Fixed-effects
                  1+year.0*AIS_grade+year.0_sq*AIS_grade+year.0_cu*AIS_grade+
                  # Random-effects
                  (1+year.0+year.0_sq+year.0_cu|subID), data=DAT2, REML=FALSE,
                  control=lmerControl(optimizer="bobyqa",
                                    optCtrl=list(maxfun=5e5)))
anova(raneff_03, cond_01)
```

As this model is an improvement beyond the unconditional models, based on the AIC reduction, we can then delve deeper into the model by passing only the best fitting model to the anova() function, which will give us the omnibus F-test for the different main-effects and interactions (similar to a more traditional ANOVA table). 

```{r}
anova(cond_01)
#Note that I actually prefer to use the Anova() function from the car package, where you can easily specify the type of SS calculation.
Anova(cond_01, type="III")
```

If we really want to drill down into the model, we can get the specific parameters estimates for the fixed-effects (similar to an OLS regression output) and the variance/covariance estimates for the random-effects by using the summary() function.

```{r}
summary(cond_01)
```

# 2. Fitting a Non-Liner Model of Change over Time
```{r}
# Negative Exponential Model ----
ggplot(DAT2, aes(x = year.0, y = rasch_FIM)) + 
  geom_line(aes(group=subID), col="grey") + 
  #stat_smooth(method="lm", formula=y~x+I(x^2), col="black", lwd=1.5, se=FALSE)+
  scale_x_continuous(name = "Time from Admission (Years)") + 
  scale_y_continuous(name = "Rasch-Scaled FIM Score (0-100)",limits=c(0,100)) +
  theme_bw() + 
  theme(axis.text=element_text(size=14, colour="black"), 
        axis.title=element_text(size=14,face="bold")) +
  theme(strip.text.x = element_text(size = 14))+ theme(legend.position="none") 

library(nlme)

set.seed(100)
neg_exp_rand_mod <- nlme(rasch_FIM ~ b_1i + 
                           (b_2i)*(exp(b_3i * year.0)),
                         data = DAT2,
                         fixed = b_1i + b_2i + b_3i ~ 1,
                         random = list(b_1i ~ 1, b_2i ~ 1, b_3i ~1),
                         groups = ~ subID,
                         start = c(80, -70, -1),
                         na.action = na.omit)
summary(neg_exp_rand_mod)

coef(neg_exp_rand_mod)
fitted(neg_exp_rand_mod)

year <- seq(from=0, to=1.5, by=0.01)
rasch_FIM <- 58.53985-48.13958*exp(-2.53036*year)
PRED <- data.frame(year, rasch_FIM)

head(DAT2)
ggplot(DAT2, aes(x = year.0, y = rasch_FIM)) + 
  geom_line(aes(group=subID), col="grey") + 
  scale_x_continuous(name = "Time from Admission (Years)") + 
  scale_y_continuous(name = "Independence (0-100)",limits=c(0,100)) +
  #facet_wrap(~AIS_grade) +
  theme_bw() + 
  theme(axis.text=element_text(size=14, colour="black"), 
        axis.title=element_text(size=14,face="bold")) +
  theme(strip.text.x = element_text(size = 14))+ theme(legend.position="none")+
  geom_line(data=PRED, aes(x=year, y=rasch_FIM), col="black", lwd=1.5)


DAT2$exp_pred <- fitted(neg_exp_rand_mod) # Save model predictions to data frame
first4<-DAT2[c(1:72),] # Second, we'll make a smaller dataset with the predictions for these 10: 


ggplot(first4, aes(x = year.0, y = rasch_FIM)) + 
  geom_line(aes(group=subID)) + facet_wrap(~subID, ncol=2)+
  geom_point(fill="grey", pch=21, size=2, stroke=1.25) + 
  scale_x_continuous(name = "Time from Admission (Years)") + 
  scale_y_continuous(name = "Independence (0-100)",limits=c(0,100))+
  geom_line(aes(y=exp_pred), lwd=1, col="purple") +
  theme_bw() + theme(axis.text=element_text(size=12, colour="black"), 
                     axis.title=element_text(size=14,face="bold")) +
  theme(strip.text.x = element_text(size = 12))+ theme(legend.position="none") 
```


# 3. A Note on Optimizers
Note that I am specifying the optimizer that we are using. Specifically, I am using the bobyqa optimizer. bobyqa is short for "Bound Approximation BY Quadratic Approximation". It is the optimizer that lme4 uses by default, but I am calling it explicitly here. You can select other optimizers using the lmerControl() function: (https://www.rdocumentation.org/packages/lme4/versions/1.1-27/topics/lmerControl). 
```{r, eval=FALSE}
control=lmerControl(optimizer="Nelder_Mead",
                                    optCtrl=list(maxfun=5e5)))
```

A good exercise is to run all of the models in this chapter using the Nelder-Mead optimizer and the BOBYQA optimizer. Do these models differ in how they estimate the fixed-effects and standard errors of the fixed-effects? Do these models differ in how they estimate the variances and covariances of the random-effects? 

You can also change the number of iterations that your maximum likelihood simulation goes through by changing the maxfun in the optCtrl argument. If you run into convergence warnings, it is a good idea to:

1. increase the number of iterations (use a minimum of 200,000 iterations), 
2. change your optimizer to see if the problem persists, and finally 
3. try simplifying the structure of your random-effects.  


For a fairly accessible discussion of these issues, I would recommend a simulation study by McCoach and colleagues (2018; https://journals.sagepub.com/doi/abs/10.3102/1076998618776348?journalCode=jebb). By simulating various multi-level data structures and then fitting the same mixed-effects models in different software packages, McCoach and colleagues found that the BOBYQA optimizer tended to give more non-convergence warnings than other optimizers/programs even though the estimates from the models were (effectively) the same.

...